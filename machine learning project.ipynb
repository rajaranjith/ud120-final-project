{
 "metadata": {
  "name": "machine learning project"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import os\nimport matplotlib.pyplot as plt\nimport sys\nimport pickle\nfrom sklearn import preprocessing\nfrom time import time\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nos.chdir('/home/himu/ml/final_project/')\nsys.path.append(\"../tools/\")\n\nfrom feature_format import featureFormat\nfrom feature_format import targetFeatureSplit\n\n\n### features_list is a list of strings, each of which is a feature name\n### first feature must be \"poi\", as this will be singled out as the label\nfeatures_list = [\"poi\"]\n\n### load the dictionary containing the dataset\ndata_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\") )\n\nprint(\"There are %d executives in Enron Dataset.\" % (length(data_dict.keys())))\nprint(data_dict.keys())\nprint(data_dict['BUY RICHARD B'])\n\n\n#OUTLIER REDUCTION FROM THE DATASET\n\n\n### remove any outliers before proceeding further\nfeatures = [\"salary\", \"bonus\"]\ndata_dict.pop('TOTAL', 0)\ndata = featureFormat(data_dict, features)\n\n### remove NAN's from dataset\noutliers = []\nfor key in data_dict:\n    val = data_dict[key]['salary']\n    if val == 'NaN':\n        continue\n    outliers.append((key, int(val)))\n\noutliers_final = (sorted(outliers,key=lambda x:x[1],reverse=True)[:4])\n### print top 4 salaries\nprint(outliers_final)\n\n\n\n\n\nfeatures = [\"salary\", \"bonus\"]\n#data_dict.pop('TOTAL', 0)\ndata = featureFormat(data_dict, features)\n\n'''\n### plot features\nfor point in data:\n    salary = point[0]\n    bonus = point[1]\n    plt.scatter( salary, bonus )\n\nplt.xlabel(\"salary\")\nplt.ylabel(\"bonus\")\nplt.show()\n'''\n\n\n#FEATURE PROCESSING\n### create new features\n### new features are: fraction_to_poi_email,fraction_from_poi_email\n\ndef dict_to_list(key,normalizer):\n    new_list=[]\n\n    for i in data_dict:\n        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n            new_list.append(0.)\n        elif data_dict[i][key]>=0:\n            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n    return new_list\n\n### create two lists of new features\nfraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\nfraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n\n### insert new features into data_dict\ncount=0\nfor i in data_dict:\n    data_dict[i][\"fraction_from_poi_email\"]=fraction_from_poi_email[count]\n    data_dict[i][\"fraction_to_poi_email\"]=fraction_to_poi_email[count]\n    count +=1\n\n    \nfeatures_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\"]    \n    ### store to my_dataset for easy export below\nmy_dataset = data_dict\n\n\n### these two lines extract the features specified in features_list\n### and extract them from data_dict, returning a numpy array\ndata = featureFormat(my_dataset, features_list)\n\n### plot new features\nfor point in data:\n    from_poi = point[1]\n    to_poi = point[2]\n    plt.scatter( from_poi, to_poi )\n    if point[0] == 1:\n        plt.scatter(from_poi, to_poi, color=\"r\", marker=\"*\")\nplt.xlabel(\"fraction of emails this person gets from poi\")\n#plt.show()\n\n#finding the ranking of features \nfeatures_list = [\"poi\", \"salary\", \"bonus\", \"fraction_from_poi_email\", \"fraction_to_poi_email\",\n                 'deferral_payments', 'total_payments', 'loan_advances', 'restricted_stock_deferred',\n                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options',\n                 'long_term_incentive', 'shared_receipt_with_poi', 'restricted_stock', 'director_fees']\ndata = featureFormat(my_dataset, features_list)\n\n### split into labels and features (this line assumes that the first\n### feature in the array is the label, which is why \"poi\" must always\n### be first in features_list\nlabels, features = targetFeatureSplit(data)\n\n### split data into training and testing datasets\nfrom sklearn import cross_validation\nfeatures_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nt0 = time()\n\nclf = DecisionTreeClassifier()\nclf.fit(features_train,labels_train)\nscore = clf.score(features_test,labels_test)\npred= clf.predict(features_test)\nprint ('accuracy: %f' %score ) \n\nprint (\"Decision tree algorithm time: %f%s\"% (round(time()-t0, 3), \"s\"))\n\n\n\nimportances = clf.feature_importances_\nimport numpy as np\nindices = np.argsort(importances)[::-1]\nprint ('Feature Ranking: ')\nfor i in range(16):\n    print( \" %d feature %s %f\" % (format(i+1,features_list[i+1],importances[indices[i]])))\n\n\n\n# ALGORITHM SELECTION AND TUNING\n\nfeatures_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", \"shared_receipt_with_poi\"]\n\n### try Naive Bayes for prediction\n\nt0 = time()\t\nclf = GaussianNB()\nclf.fit(features_train, labels_train)\npred = clf.predict(features_test)\naccuracy = accuracy_score(pred,labels_test)\nprint(accuracy)\n\nprint (\"NB algorithm time: %f%s\"% (round(time()-t0, 3), \"s\"))\n\n### trying Decision tree with different minimum_split\n\n\nprint (\"SPLIT\\tACCURACY\\tPRECISION\\tRECALL\\tTIME\")\nfor i in range (2,10): \n    t0 = time()\n    clf = DecisionTreeClassifier(min_samples_split=i)\n    clf.fit(features_train,labels_train)\n    pred = clf.predict(features_test)\n    accuracy = accuracy_score(pred,labels_test)\n    precision = precision_score(labels_test,pred)\n    recall = recall_score(labels_test,pred)\n    print(\"%d\\t%0.4f\\t\\t%0.4f\\t\\t%0.4f\\t%0.4f\" % (i,accuracy,precision,recall,time()-t0))\n\n\n\n#ANALYSIS VALIDATION & PERFORMANCE\n\n### features_list is a list of strings, each of which is a feature name\n### first feature must be \"poi\", as this will be singled out as the label\nfeatures_list = [\"poi\", \"fraction_from_poi_email\", \"fraction_to_poi_email\", 'shared_receipt_with_poi']\n\n\n### store to my_dataset for easy export below\nmy_dataset = data_dict\n\n\n### these two lines extract the features specified in features_list\n### and extract them from data_dict, returning a numpy array\ndata = featureFormat(my_dataset, features_list)\n\n\n### split into labels and features (this line assumes that the first\n### feature in the array is the label, which is why \"poi\" must always\n### be first in features_list\nlabels, features = targetFeatureSplit(data)\n\n\n### machine learning goes here!\n### please name your classifier clf for easy export below\n\n### deploying feature selection\nfrom sklearn import cross_validation\nfeatures_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)\n\n### use KFold for split and validate algorithm\nfrom sklearn.cross_validation import KFold\nkf=KFold(len(labels),3)\nfor train_indices, test_indices in kf:\n    #make training and testing sets\n    features_train= [features[ii] for ii in train_indices]\n    features_test= [features[ii] for ii in test_indices]\n    labels_train=[labels[ii] for ii in train_indices]\n    labels_test=[labels[ii] for ii in test_indices]\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nt0 = time()\n\nclf = DecisionTreeClassifier()\nclf.fit(features_train,labels_train)\nscore = clf.score(features_test,labels_test)\nprint(\"accuracy before tuning %f\"% score)\n\nprint( \"Decision tree algorithm time: %d %s\"% (round(time()-t0, 3), \"s\"))\n\n\n### use manual tuning parameter min_samples_split\nt0 = time()\nclf = DecisionTreeClassifier(min_samples_split=5)\nclf = clf.fit(features_train,labels_train)\npred= clf.predict(features_test)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nacc=accuracy_score(labels_test, pred)\n\nprint (\"Validating algorithm:\")\nprint (\"accuracy after tuning = %f\"% acc)\n\n# function for calculation ratio of true positives\n# out of all positives (true + false)\nprint ('precision = %lf'% precision_score(labels_test,pred))\n\n# function for calculation ratio of true positives\n# out of true positives and false negatives\nprint ('recall = %lf'% recall_score(labels_test,pred))\n\n\n### dump your classifier, dataset and features_list so\n### anyone can run/check your results\npickle.dump(clf, open(\"my_classifier.pkl\", \"w\") )\npickle.dump(data_dict, open(\"my_dataset.pkl\", \"w\") )\npickle.dump(features_list, open(\"my_feature_list.pkl\", \"w\") ) ",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}